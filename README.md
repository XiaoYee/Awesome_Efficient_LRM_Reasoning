# Awesome-Efficient-LRM-Reasoning

[![Awesome](https://awesome.re/badge.svg)](https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/github/last-commit/XiaoYee/Awesome_Efficient_LRM_Reasoning?color=green) 

Must Read papers about Awesome-Efficient-LRM-Reasoning

---

## ğŸ”” News

[2025-03] We create this repository to maintain a paper list on Awesome-Efficient-LRM-Reasoning.

---

## ğŸ”¥ Table of Contents

[TOC]

---

## ğŸ“œContent

### Abstract
- [Awesome-Efficient-LRM-Reasoning](#awesome-efficient-lrm-reasoning)
  - [ğŸ”” News](#-news)
  - [ğŸ”¥ Table of Contents](#-table-of-contents)
  - [ğŸ“œContent](#content)
    - [Abstract](#abstract)
    - [Introduction](#introduction)
  - [ğŸŒ„ Papers](#-papers)
    - [ğŸ¤– Reasoning Inefficiency: Definition, Patterns and Challenges](#-reasoning-inefficiency-definition-patterns-and-challenges)
      - [Patterns of Reasoning Inefficiency](#patterns-of-reasoning-inefficiency)
      - [Unique Challenges for Efficient Reasoning in the Era of LRMs](#unique-challenges-for-efficient-reasoning-in-the-era-of-lrms)
    - [ğŸ’­ Efficient Reasoning during Inference](#-efficient-reasoning-during-inference)
      - [Length Budgeting](#length-budgeting)
      - [System Switch](#system-switch)
      - [Model Switch](#model-switch)
      - [Parallel Search](#parallel-search)
    - [ğŸ’« Efficient Reasoning with SFT](#-efficient-reasoning-with-sft)
      - [Reasoning Chain Compression](#reasoning-chain-compression)
      - [Latent-Space SFT](#latent-space-sft)
    - [ğŸ§© Efficient Reasoning with Reinforcement Learning](#-efficient-reasoning-with-reinforcement-learning)
      - [Efficient Reinforcement Learning with Length Reward](#efficient-reinforcement-learning-with-length-reward)
      - [Efficient Reinforcement Learning without Length Reward](#efficient-reinforcement-learning-without-length-reward)
    - [ğŸ’¬ Efficient Reinforcement Learning without Length Reward](#-efficient-reinforcement-learning-without-length-reward)
      - [Pretraining with Latent Space](#pretraining-with-latent-space)
      - [Subquadratic Attention](#subquadratic-attention)
      - [Linearization](#linearization)
      - [Efficient Reasoning with Subquadratic Attention](#efficient-reasoning-with-subquadratic-attention)
    - [ğŸ”– Future Directions](#-future-directions)
      - [Efficient Multimodal Reasoning and Video Reasoning](#efficient-multimodal-reasoning-and-video-reasoning)
      - [Efficient Test-time Scaling and Infinity Thinking](#efficient-test-time-scaling-and-infinity-thinking)
      - [Efficient and Trustworthy Reasoning](#efficient-and-trustworthy-reasoning)
      - [Building Efficient Reasoning Applications](#building-efficient-reasoning-applications)
      - [Evaluation and Benchmark](#evaluation-and-benchmark)
  - [ğŸ‰ Contribution](#-contribution)
    - [Contributing to this paper list](#contributing-to-this-paper-list)
    - [Contributors](#contributors)

**Recent Large Reasoning Models (LRMs)**, such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long and inefficient reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. 
This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. 
In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm.
We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. 
To support ongoing development, we also maintain a real-time [GitHub repository](https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning) tracking recent progress in the field.
We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.

### Introduction




---

## ğŸŒ„ Papers

### ğŸ¤– Reasoning Inefficiency: Definition, Patterns and Challenges

#### Patterns of Reasoning Inefficiency

#### Unique Challenges for Efficient Reasoning in the Era of LRMs

### ğŸ’­ Efficient Reasoning during Inference

#### Length Budgeting

#### System Switch

#### Model Switch

#### Parallel Search


### ğŸ’« Efficient Reasoning with SFT

#### Reasoning Chain Compression

#### Latent-Space SFT

### ğŸ§© Efficient Reasoning with Reinforcement Learning

#### Efficient Reinforcement Learning with Length Reward

#### Efficient Reinforcement Learning without Length Reward

### ğŸ’¬ Efficient Reinforcement Learning without Length Reward

#### Pretraining with Latent Space

#### Subquadratic Attention

#### Linearization

#### Efficient Reasoning with Subquadratic Attention

### ğŸ”– Future Directions

#### Efficient Multimodal Reasoning and Video Reasoning

#### Efficient Test-time Scaling and Infinity Thinking

#### Efficient and Trustworthy Reasoning

#### Building Efficient Reasoning Applications

#### Evaluation and Benchmark




---
## ğŸ‰ Contribution

### Contributing to this paper list

â­" **Join us in improving this repository!** If you know of any important works we've missed, please contribute. Your efforts are highly valued!   "

### Contributors

<a href="https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=XiaoYee/Awesome_Efficient_LRM_Reasoning" />
</a>
